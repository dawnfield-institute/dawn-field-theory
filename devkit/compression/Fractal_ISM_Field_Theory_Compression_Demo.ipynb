{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5fcd622",
   "metadata": {},
   "source": [
    "# Fractal Compression with ISM: A Novel Field-Theoretic Algorithm\n",
    "\n",
    "Welcome! This notebook introduces a **brand new approach to symbolic data compression** developed in the context of Dawn Field Theory and infodynamics.\n",
    "\n",
    "You will learn how to:\n",
    "- Break data into symbolic chunks (fractal/codebook style)\n",
    "- Attach *intrinsic structural metadata* (ISM) to each chunk\n",
    "- Analyze entropy and structure across your data\n",
    "- Run and experiment with a blueprint implementation\n",
    "\n",
    "**This is the first implementation of ISM-style field-theoretic compression in this repository.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d85c05",
   "metadata": {},
   "source": [
    "**Step 1: Import Requirements**\n",
    "\n",
    "Run this cell to import necessary Python libraries.\n",
    "If you get an error, run `pip install numpy matplotlib` in your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170a966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import hashlib\n",
    "import zlib\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e23e3b",
   "metadata": {},
   "source": [
    "**Step 2: Data Chunking**\n",
    "\n",
    "We'll break your data into equal-sized symbolic chunks.\n",
    "You can change `chunk_size` in the next cell to see its impact.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a4ddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkify(data, chunk_size=256):\n",
    "    chunks = [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]\n",
    "    if len(chunks[-1]) < chunk_size:\n",
    "        chunks[-1] += b'\\x00' * (chunk_size - len(chunks[-1]))\n",
    "    return chunks\n",
    "\n",
    "# Demo: try with simple pattern\n",
    "test_data = b'ABCD1234' * 16\n",
    "chunks = chunkify(test_data, chunk_size=8)\n",
    "print(f'Chunks: {len(chunks)}')\n",
    "for i, c in enumerate(chunks):\n",
    "    print(f'Chunk {i}:', c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d6924c",
   "metadata": {},
   "source": [
    "**Step 3: Fractal Codebook Construction**\n",
    "\n",
    "Now we build a codebook of unique chunks, and represent the data as a sequence of codebook indices.\n",
    "This mirrors symbolic collapse and recursion in field theory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a665bef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_chunk(chunk):\n",
    "    return hashlib.sha256(chunk).digest()[:8]\n",
    "\n",
    "def build_fractal_index(chunks):\n",
    "    index = {}\n",
    "    codebook = []\n",
    "    sequence = []\n",
    "    for chunk in chunks:\n",
    "        h = hash_chunk(chunk)\n",
    "        match_idx = index.get(h)\n",
    "        if match_idx is None:\n",
    "            match_idx = len(codebook)\n",
    "            index[h] = match_idx\n",
    "            codebook.append(chunk)\n",
    "        sequence.append(match_idx)\n",
    "    return codebook, sequence\n",
    "\n",
    "codebook, sequence = build_fractal_index(chunks)\n",
    "print(f'Unique codebook entries: {len(codebook)}')\n",
    "print(f'Sequence: {sequence}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d999873",
   "metadata": {},
   "source": [
    "**Step 4: ISM Metadata Extraction**\n",
    "\n",
    "For each codebook entry, compute intrinsic structural metadata:\n",
    "\n",
    "- Entropy\n",
    "- Centroid\n",
    "- Power\n",
    "- Dominant Frequency\n",
    "\n",
    "Run the next cell to compute and visualize these properties.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46040600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ism_metadata(codebook):\n",
    "    metadata = []\n",
    "    for chunk in codebook:\n",
    "        arr = np.frombuffer(chunk, dtype=np.uint8)\n",
    "        entropy = -np.sum((p := np.bincount(arr, minlength=256) / len(arr)) * np.log2(p + 1e-10))\n",
    "        centroid = np.mean(arr)\n",
    "        power = np.mean(arr ** 2)\n",
    "        fft = np.fft.rfft(arr)\n",
    "        dom_freq = np.argmax(np.abs(fft))\n",
    "        metadata.append((entropy, centroid, power, dom_freq))\n",
    "    return metadata\n",
    "\n",
    "metadata = ism_metadata(codebook)\n",
    "print('ISM Metadata (first 3):', metadata[:3])\n",
    "\n",
    "# Plot entropy and centroid\n",
    "ent = [m[0] for m in metadata]\n",
    "cent = [m[1] for m in metadata]\n",
    "plt.figure(figsize=(6,2))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(ent, marker='o'); plt.title('Entropy'); plt.xlabel('Chunk'); plt.ylabel('H')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(cent, marker='o'); plt.title('Centroid'); plt.xlabel('Chunk'); plt.ylabel('Mean')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f3930f",
   "metadata": {},
   "source": [
    "**Step 5: Full Compression & Decompression**\n",
    "\n",
    "Now let's use the codebook and ISM to compress and restore any data block.\n",
    "\n",
    "- Run the next cell on different data (structured, random, etc.)\n",
    "- Confirm that decompression is exact (`restored == data`)\n",
    "- Inspect compression ratio and codebook statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee0e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fractal_compress(data, chunk_size=256):\n",
    "    chunks = chunkify(data, chunk_size)\n",
    "    codebook, sequence = build_fractal_index(chunks)\n",
    "    metadata = ism_metadata(codebook)\n",
    "    sequence_bytes = np.array(sequence, dtype=np.uint16).tobytes()\n",
    "    codebook_bytes = b''.join(codebook)\n",
    "    meta_bytes = b''.join(struct.pack('>ffff', *m) for m in metadata)\n",
    "    packed = struct.pack('>I', len(data)) + struct.pack('>H', chunk_size) + struct.pack('>H', len(codebook)) + codebook_bytes + sequence_bytes + meta_bytes\n",
    "    return zlib.compress(packed), codebook, sequence, metadata\n",
    "\n",
    "def fractal_decompress(packed):\n",
    "    raw = zlib.decompress(packed)\n",
    "    original_len = struct.unpack('>I', raw[:4])[0]\n",
    "    chunk_size = struct.unpack('>H', raw[4:6])[0]\n",
    "    count = struct.unpack('>H', raw[6:8])[0]\n",
    "    offset = 8\n",
    "    codebook = [raw[offset+i*chunk_size:offset+(i+1)*chunk_size] for i in range(count)]\n",
    "    offset += count * chunk_size\n",
    "    meta_size = count * 16\n",
    "    sequence_length = (len(raw) - offset - meta_size) // 2\n",
    "    sequence = np.frombuffer(raw[offset:offset + sequence_length * 2], dtype=np.uint16)\n",
    "    meta_offset = offset + sequence_length * 2\n",
    "    metadata = [struct.unpack('>ffff', raw[meta_offset+i*16:meta_offset+(i+1)*16]) for i in range(count)]\n",
    "    restored = b''.join(codebook[i] for i in sequence)\n",
    "    return restored[:original_len], codebook, sequence, metadata\n",
    "\n",
    "# Try the pipeline\n",
    "data = (b'ABCD1234' * 1024) + (b'XYZ9876' * 512)\n",
    "compressed, codebook, sequence, metadata = fractal_compress(data, chunk_size=32)\n",
    "restored, _, _, _ = fractal_decompress(compressed)\n",
    "print('Compression ratio:', len(compressed)/len(data))\n",
    "print('Restored == Original:', restored == data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690be6e5",
   "metadata": {},
   "source": [
    "**Step 6: Explore & Extend**\n",
    "\n",
    "Try running the above pipeline on:\n",
    "- Larger data files (try images, audio, text)\n",
    "- Change `chunk_size` or try random data\n",
    "- Visualize all ISM metrics\n",
    "- Compare with classic compressors (zlib, bz2, lzma)\n",
    "\n",
    "---\n",
    "\n",
    "**Blueprint for Expansion:**\n",
    "\n",
    "- Add recursive or hierarchical chunking\n",
    "- Use ISM metrics for adaptive transforms\n",
    "- Integrate with field-theoretic tools and validation\n",
    "\n",
    "This notebook is an open blueprintâ€”extend and adapt to push the boundaries of field-theoretic compression!\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
